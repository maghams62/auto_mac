#!/usr/bin/env python3
"""
Focused test for Explain Pipeline functionality.
Tests the core explain features without full agent initialization.
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__))))

import logging
from unittest.mock import Mock, patch
from src.services.explain_pipeline import ExplainPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_explain_pipeline_core():
    """Test core explain pipeline functionality with mocked components."""
    print("\n" + "="*80)
    print("TEST: EXPLAIN PIPELINE CORE FUNCTIONALITY")
    print("="*80)

    # Create mock registry
    mock_registry = Mock()

    # Mock search_documents response
    mock_search_result = {
        "results": [{
            "doc_path": "/test/path/tesla_autopilot_test.txt",
            "doc_title": "Tesla Autopilot Technical Overview",
            "content_preview": "Tesla Autopilot is an advanced driver-assistance system...",
            "relevance_score": 0.85
        }]
    }

    # Mock extract_section response
    mock_extract_result = {
        "extracted_text": """
        Tesla Autopilot Technical Overview
        ===================================

        Summary
        -------
        Tesla Autopilot is an advanced driver-assistance system that uses cameras, radar, and ultrasonic sensors to enable semi-autonomous driving capabilities.

        Key Features:
        - Adaptive cruise control
        - Lane keeping assistance
        - Automatic lane changes
        - Highway autopilot
        - Navigate on Autopilot
        """
    }

    # Mock synthesize_content response
    mock_synth_result = {
        "synthesized_content": """## Tesla Autopilot Overview

Tesla Autopilot is an advanced driver-assistance system that enhances driving safety and convenience through semi-autonomous capabilities.

### Key Features
- **Adaptive Cruise Control**: Maintains safe following distance
- **Lane Keeping Assistance**: Keeps vehicle centered in lane
- **Automatic Lane Changes**: Smooth lane transitions when safe
- **Highway Autopilot**: Handles highway driving scenarios
- **Navigate on Autopilot**: Complete trip automation

*Document: Tesla Autopilot Technical Overview (similarity: 0.85)*
*Word count: 87*""",
        "word_count": 87
    }

    # Configure mocks
    mock_registry.execute_tool.side_effect = [
        mock_search_result,  # search_documents
        mock_extract_result, # extract_section
        mock_synth_result    # synthesize_content
    ]

    # Create pipeline
    pipeline = ExplainPipeline(mock_registry)

    # Test 1: Basic explain functionality
    print("Testing basic explain functionality...")
    result = pipeline.execute("explain Tesla quarterly review")

    if result.get("success"):
        print("âœ“ PASS: Pipeline executed successfully")
        print(f"  Summary: {result['summary'][:100]}...")
        print(f"  Doc Title: {result['doc_title']}")
        print(f"  Word Count: {result['word_count']}")
        print(f"  Telemetry steps: {result['telemetry']['pipeline_steps']}")
    else:
        print(f"âœ— FAIL: Pipeline failed: {result}")
        return False

    # Test 2: Topic extraction
    print("\nTesting topic extraction...")
    test_cases = [
        ("explain Edgar Allan Poe", "Edgar Allan Poe"),
        ("summarize machine learning", "machine learning"),
        ("describe quantum physics", "quantum physics"),
        ("tell me about artificial intelligence", "artificial intelligence"),
        ("what is blockchain", "blockchain"),
        ("explain the Tesla quarterly review file", "the Tesla quarterly review")
    ]

    for input_text, expected in test_cases:
        result = pipeline._extract_topic(input_text)
        if result == expected:
            print(f"âœ“ PASS: '{input_text}' -> '{result}'")
        else:
            print(f"âœ— FAIL: '{input_text}' expected '{expected}', got '{result}'")
            return False

    # Test 3: Synthesis style determination
    print("\nTesting synthesis style determination...")
    style_tests = [
        ("explain in detail", "comprehensive"),
        ("summarize briefly", "concise"),
        ("compare these options", "comparative"),
        ("timeline of events", "chronological"),
        ("what happened", "concise")  # default
    ]

    for task, expected_style in style_tests:
        style = pipeline._determine_synthesis_style(task)
        if style == expected_style:
            print(f"âœ“ PASS: '{task}' -> '{style}'")
        else:
            print(f"âœ— FAIL: '{task}' expected '{expected_style}', got '{style}'")
            return False

    # Test 4: Error handling - no search results
    print("\nTesting error handling...")
    mock_registry.execute_tool.side_effect = [{"error": "No documents found"}]
    error_result = pipeline.execute("explain nonexistent document")

    if not error_result.get("success") and error_result.get("error"):
        print("âœ“ PASS: Error handling works correctly")
    else:
        print(f"âœ— FAIL: Error handling failed: {error_result}")
        return False

    print("\nâœ“ ALL EXPLAIN PIPELINE TESTS PASSED")
    return True


def test_slash_command_integration():
    """Test slash command integration with explain pipeline."""
    print("\n" + "="*80)
    print("TEST: SLASH COMMAND INTEGRATION")
    print("="*80)

    # Import here to avoid langchain issues
    try:
        from src.ui.slash_commands import SlashCommandHandler
        from src.utils import load_config
    except Exception as e:
        print(f"Skipping slash command test due to import issue: {e}")
        return True

    try:
        config = load_config()

        # Mock agent registry
        with patch('src.agent.agent_registry.AgentRegistry') as mock_registry_class:
            mock_registry = Mock()
            mock_registry_class.return_value = mock_registry

            # Create handler
            handler = SlashCommandHandler(mock_registry, config)

            # Test /explain command
            is_command, result = handler.handle("/explain Tesla autopilot")
            if is_command and result.get("agent") == "explain":
                print("âœ“ PASS: /explain command routed correctly")
            else:
                print(f"âœ— FAIL: /explain command routing failed: {result}")
                return False

            # Test /files explain command
            is_command, result = handler.handle("/files explain machine learning")
            if is_command and result.get("agent") == "explain":
                print("âœ“ PASS: /files explain command routed correctly")
            else:
                print(f"âœ— FAIL: /files explain command routing failed: {result}")
                return False

        print("\nâœ“ SLASH COMMAND INTEGRATION TESTS PASSED")
        return True

    except Exception as e:
        print(f"âœ— FAIL: Slash command test error: {e}")
        return False


if __name__ == "__main__":
    print("EXPLAIN COMMAND VERIFICATION SUITE")
    print("="*80)

    results = {
        "Explain Pipeline Core": test_explain_pipeline_core(),
        "Slash Command Integration": test_slash_command_integration(),
    }

    print("\n" + "="*80)
    print("FINAL RESULTS")
    print("="*80)

    passed = 0
    total = len(results)

    for test_name, result in results.items():
        status = "âœ“ PASS" if result else "âœ— FAIL"
        print(f"  {status} - {test_name}")
        if result:
            passed += 1

    print(f"\nPassed: {passed}/{total} ({int(passed/total*100) if total > 0 else 0}%)")

    if passed == total:
        print("\nğŸ‰ ALL TESTS PASSED - Explain functionality verified!")
    else:
        print("\nâŒ Some tests failed - Check implementation")
